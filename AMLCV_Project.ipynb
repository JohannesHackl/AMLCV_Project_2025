{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fccac6",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision.models.segmentation import DeepLabV3_ResNet50_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Reproducibility (Crucial for MSc reports)\n",
    "torch.manual_seed(12)\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab78c3",
   "metadata": {},
   "source": [
    "# The Dataset (Cityscapes Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# Rresize to 256x512 to speed up training \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 512)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.PILToTensor(), # Converts to [0, 255] int tensor\n",
    "])\n",
    "\n",
    "# Load Dataset (assuming data is in ./data/cityscapes)\n",
    "# 'quality_mode'='fine' is the standard segmentation task\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    train_dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
    "                               target_type='semantic', transform=data_transforms, \n",
    "                               target_transform=target_transforms)\n",
    "    \n",
    "    val_dataset = Cityscapes('./data/cityscapes', split='val', mode='fine',\n",
    "                             target_type='semantic', transform=data_transforms, \n",
    "                             target_transform=target_transforms)\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Ensure Cityscapes is downloaded and extracted to ./data/cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4dbb41",
   "metadata": {},
   "source": [
    "# Visualize Data (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, index=0):\n",
    "    img, mask = dataset[index]\n",
    "    \n",
    "    # Undo normalization for visualization\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    img = inv_normalize(img)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    \n",
    "    # Mask is [1, H, W], squeeze to [H, W]\n",
    "    ax[1].imshow(mask.squeeze(), cmap='jet') \n",
    "    ax[1].set_title(\"Ground Truth Mask\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(train_dataset, index=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf7cff",
   "metadata": {},
   "source": [
    "# Model Selection & Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f28efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Pre-trained DeepLabV3 with ResNet50 backbone\n",
    "# Weights.DEFAULT loads the best available pre-trained weights\n",
    "model = models.segmentation.deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# 2. Adapt the Classifier Head\n",
    "# Cityscapes has 19 valid classes. (around 30 total, but usually mapped to 19 for training)\n",
    "# Here we assume the raw dataset output, or we map 255 to ignore).\n",
    "num_classes = 34 # Raw cityscapes classes usually go up to 33 + background\n",
    "\n",
    "# DeepLab has a 'classifier' module. The last layer is classifier[4]\n",
    "model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# If using auxiliary loss (helps training convergence), adapt that too\n",
    "model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384155b",
   "metadata": {},
   "source": [
    "# Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c429178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 255 is the 'void' class in Cityscapes (borders, ego vehicle, etc.)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255) \n",
    "\n",
    "# Optimization: Adam or SGD\n",
    "# Learning rate is a hyperparameter \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04a885",
   "metadata": {},
   "source": [
    "# The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdede39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).long().squeeze(1) # [Batch, H, W]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)['out'] # DeepLab returns a dictionary\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "loss_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    loss_history.append(loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plotting Convergence \n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53111e13",
   "metadata": {},
   "source": [
    "# Evaluation (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, true_mask, num_classes):\n",
    "    ious = []\n",
    "    pred_mask = pred_mask.view(-1)\n",
    "    true_mask = true_mask.view(-1)\n",
    "    \n",
    "    # Ignore void class (255)\n",
    "    valid = true_mask != 255\n",
    "    pred_mask = pred_mask[valid]\n",
    "    true_mask = true_mask[valid]\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred_mask == cls\n",
    "        target_inds = true_mask == cls\n",
    "        \n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        \n",
    "        if union > 0:\n",
    "            ious.append(intersection / union)\n",
    "            \n",
    "    return np.mean(ious)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "model.eval()\n",
    "total_iou = 0\n",
    "batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in val_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).squeeze(1)\n",
    "        \n",
    "        outputs = model(images)['out']\n",
    "        preds = torch.argmax(outputs, dim=1) # Convert logits to class index\n",
    "        \n",
    "        batch_iou = 0\n",
    "        for i in range(len(images)):\n",
    "            batch_iou += calculate_iou(preds[i], masks[i], num_classes)\n",
    "        \n",
    "        total_iou += batch_iou / len(images)\n",
    "        batches += 1\n",
    "        \n",
    "print(f\"Mean IoU on Validation Set: {total_iou/batches:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699182b",
   "metadata": {},
   "source": [
    "# Critical Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
